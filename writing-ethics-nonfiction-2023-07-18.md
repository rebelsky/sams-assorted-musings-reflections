---
title: Ethics of writing nonfiction
number: NNN
tags: [On writing](index-on-writing), [ChatGPT and LLMs](index-generative-ai)
blurb: At least some ethical issues
version: 0.1
released: 2023-07-19
current: 
---
Like many academics, I've been thinking a lot about ChatGPT, Large Language Models (LLMs), and other types of generative AI.  One can easily identify a host of issues with students using such systems, particularly that students wo use such systems may not get appropriate practice writing.  While writing is hard, it should also be fun.  But it's not necessarily fun until you get good at it.  And it takes practice to get good, or at least it usually takes practice.

I've been thinking about a different issue, one that ties most directly to the use of LLMs in Computer Programming.  And yes, there are LLMs that are designed to help programmers.  Many were often trained on code from GitHub, Stack Overflow, and other easily readable places on the Interweb.

That raises an important ethical and legal issue: The code that GitHub Copilot and other LLM-based programming tools generates potentially violates the licenses that accompany much of the code on the Interweb.  Free and Open-Source Software (FOSS) [1] licenses usually require, at minimum, that anyone who creates derivative works must cite the originals and mention the accompany license [2].  These tools do not insert the appropriate license.  It's not even clear that they can; the people who built the models didn't try to ensure that licenses remained tied to code.

Therefore, it may be illegal to use code generated by these systems.  That's still an open question, one to be settled by the courts.  I'm pretty sure that many major corporations tell their programmers not to use these tools just for that reason.

Because the developers of these tools seem to have unethically gathered the information used to train the tools, it may also be unethical to use such tools.  At the very least, that's an issue that every programmer who considers using such tools should consider.  And yes, it's something I'm going to ask my students to consider this fall, even as I ask them to use these tools in some cases (and to av oid them in others).

And that got me wondering ... is it similarly unethical for a student writing an essay to use ChatGPT, since ChatGPT has been trained on works without the permission of authors.  For example, from what I've heard, large amounts of the primary dataset come from [An Archive of Our Own](https://archiveofourown.org), potentially in violation of their terms of service.  Sarah Silverman, Richard Kadrey, and other writers have also opened [a class-action lawsuit](https://llmlitigation.com) about this very issue.

In programming---at least as it's taught at Grinnell and encoded in FOSS licensees---there's a clear ethical issue at stake: _You don't use someone else's work without crediting them appropriately._

In writing---at least in academic nonfiction writing---there's a similar issue at stake: _You don't quote or paraphrase someone else without citing them appropriately._  Of course, appropriate citation differs from form to form.  In some cases, full MLA style is necessary.  But I've seen books about writing whose main mode of citation is on order of "In an article in _The New York Times_, David Foster Wallace writes ...."

At first glance, it might seem like a student writer who uses ChatGPT could just cite ChatGPT.  Perhaps they write in the acknowledgements section that they used ChatGPT to generate a draft, which they then rewrote.  Perhaps something more explicit, such as "As ChatGPT suggests,"

> [I]t is not okay to use someone else's writing without citing them. Using someone else's work without giving proper credit is a form of plagiarism, which is both unethical and can be illegal in certain circumstances. Plagiarism involves presenting someone else's ideas, words, or work as your own without acknowledgment. [3,5]

As I said, at first glance, this might seem okay.

But it's not.

Why not?

Because you're not just using ChatGPT's words.  You're using the words of whoever's text contributed to the section of ChatGPT's model that generated those words.  And we don't know who it is.  I suppose we could cite "Anonymous" or "ChatGPT database", but that undermines a few of the key reasons we cite: It doesn't give credit where credit is due and it doesn't permit a scholar to trace the development of ideas [7].

Will those issues stop students from using ChatGPT or other large language models?  Probably not.  However, it strikes me that it's still worthwhile to get students to consider the ethical issues, perhaps in a broader discussion of why we cite.

I wonder when I'll have the opportunity to raise this issue with students.

---

[1] Also "Free, Libre, and Open-Source Software".

[2] Some go further, such as insisting that the derivative code be released under the same license.

[3] ChatGPT.  18 July 2023.  Online conversation at <https://chat.openai.com>.  First paragraph of response to the question "Is it okay to use someone else's writing without citing them?" [4]

[4] That was attempt to invent a citation style for a ChatGPT.  I did find [a guide to citing ChatGPT on Scribber](https://www.scribbr.com/ai-tools/chatgpt-citations/).  See the next endnote for that example.

[5] "Is it okay to use someone else's writing without citing them?" prompt. ChatGPT, 24 May 2023 free version, OpenAI, 18 July 2023, chat.openai.com. [6]

[6] Why don't MLA URLs include a protocol?

[7] I'm surprised that "you can't trace ideas" does not always appear as a key reason to cite.
