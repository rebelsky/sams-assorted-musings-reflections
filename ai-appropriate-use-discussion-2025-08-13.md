---
title: Discussing the appropriate use of generative AI in the classroom
number: 1362
tags: [Teaching](index-teaching)
blurb: A possible approach
version: 1.1
released: 2025-08-13
current: 2025-08-13
---
Earlier this summer, I participated in a workshop on generative AI and writing, led by two of my amazing colleagues in the writing center. The focus of the workshop was primarily on traditional kinds of writing, such as essays and short reading responses. While I use reading responses in my classes, most of what my students write is code, programmer documentation, and, at times, user documentation. That's not to say that I don't teach essay writing from time to time. However, it happens most frequently in Tutorial. And, for whatever reason, I think a lot about writing. Perhaps it's because I write regularly.

Although the focus of the workshop was not directly connected to my regular teaching, I found it useful to consider implications of generative AI for writing and other academic work. That intersection is quite complex.

I also emerged from the workshop with a new idea for my CS classes. One of our activities, which came from Stephanie Hanson at the University of Minnesota, asked us to consider a series of scenarios involving the use of generative AI, along with course outcomes, and to explore how appropriate we thought those uses were in the context of the course outcomes. As you might expect, Hanson designed the activity to use with students. Nonetheless, it was also helpful for us to go through it as faculty. One of my favorite scenarios to discuss went a follows.

> Student A puts her partner's paper into Gen AI and asks for feedback to make the paper better. When the students meet to share their peer review feedback, Student A shares her own ideas about her partner's paper, and also adds that she got extra feedback from GenAI to share.

Most of the participants seemed to think that this action was okay. But a few of us suggested that it's not acceptable to feed the beast without permission from the person whose (copyrighted) text you've used.

In any case, one of my most direct outcomes from the workshop was a draft of my own version of that activity (or at least the handout for that activity). I share it here in case others want to provide feedback or, perhaps, try it themselves.

---

Background
----------

As you may know, Generative AI tools like ChatGPT and Copilot have found increasing use as aids to programming. As you may know from my statement on AI, I am concerned about how regular use of AI to generate program code can affect your learning of key course concepts, such as how to decompose a complex problem, abstract away from an underlying representation or implementation, or design an algorithm. Still, there are reasonable and appropriate uses of AI, even in this class.

To help us develop a shared model of appropriate AI use, we will consider a variety of situations and reflect upon whether or not we deem such uses acceptable, as well as the reasons for our answers.

First, reflect on the learning objectives and goals for the course.

Next, read each scenario and mark your opinion as to where it falls on the acceptable (OK) to unacceptable (NOT OK) scale. You may want to take a few notes regarding your rationale. Note that you should assume that the use of generative AI has been cited (as it should be).

After that, compare your answers to those of your group mates. Please focus on the ones in which there is a difference of opinion or where you think my opinion will be significantly different than yours. 

Once we've completed the small-group conversations, we'll go through the list as a class. 

If you finish your small-group conversation early, try to brainstorm some other scenarios we might consider.

Learning objectives
-------------------

_These will, of course, depend on the particular course. I'm including some broad learning outcomes that might be appropriate in most CS classes._

**Computational thinking**. This course should enhance your skills in various aspects of computational thinking, including decomposition, data abstraction, procedural abstraction, and pattern identification.

**Software development**. This course should enhance your skills in writing and developing programs, including turning algorithms into code, developing readable and well-structured code, debugging, documentation, and testing.

**Collaboration and communication**. This course should enhance your ability to work with other people and to communicate with those who may think differently. Most of the learning will be experiential, as you collaborate both formally (e.g., in lab) and informally (e.g., helping each other on assignments).

**Ethical issues**. This course should encourage you to think more carefully about the impacts of computer technology and give you tools to reflect on potential benefits and harms.

_Yeah, that seems like a good core set. They apply to all the CS courses I teach, including our upper-level course on algorithms. Are there courses they don't apply to? Arguably, our upper-level course in the Theory of Computation (which Grinnell calls *Automata, Formal Languages, and Computational Complexity*) need not build skills in software development. But it should have some coverage of the other three primary topics._

Scenarios
---------

_I haven't phrased these in depth yet. For example, "Brainstorming project ideas" could be phrased as something like "Students A, B, and C are working together on the final project for this class, but are having trouble coming up with good ideas for the project. They feed the assignment into a GenAI tool, describe their strengths and weaknesses, and ask it to come up with something that they should be able to do within the allotted time."_

Braingstorming project ideas.

Decomposing a complex problem into smaller parts.

Analyzing the efficiency of an algorithm

Giving sample code for a structure or procedure you know about, but have forgotten the details.

Designing an algorithm, but not the code for it.

Generating test cases.

Writing documentation.

Writing "easy" code, such as setters or getters.

Explaining a complex piece of code that you don't understand with a prompt like "Can you explain the following code?"

* If the code comes from an assigned reading.
* If the code is part of the "starter code" for an assignment.
* If you found the code on the Interweb.

Identifying possible syntax errors in a program.

Explaining an error message you've received.

Tracking down bugs in a program.

Tracking down memory leaks in a program.

Translating a program or part of a program from one programming language to another.

Translating a natural-language description of an algorithm to code.

Summarizing an assigned reading.

Translating an assigned reading into another language.

Generating flashcards for an assigned reading.

Generating sample problems for an assigned reading so that you can test your knowledge and understanding.

Generating sample problems for a learning outcome to help you study for an exam.

Improving an algorithm that you've written that already works.

Improving code that you've written that already works.

Acknowledgements
----------------

This exercise is inspired by a slide entitled "Appropriate Uses of AI Discussion" in a talk given by Darren K. LaScotte, Ph.D., at a Summer 2024 workshop sponsored by the Minnesota English Language Program. LaScotte cites (Hanson, 2024) for the ideas, but I was not able to track down that citation. In an email message, LaScotte noted that Stephanie Hanson had created the activity and worksheet, but had not (yet) published anything about it.

---

Additional notes
----------------

I have some work to do on this before it's ready for prime time (or at least for class use). Here are some questions I've found myself asking.

_Would it be useful to compare writing an essay to writing code?_ I don't think folks would object to using a GenAI tool to review sentence structures. Is reviewing programming techniques the same thing?

_When in the semester should we have this conversation?_ For CSC-151, I can see having the conversation about the time we have the pair programming conversation---somewhere between weeks three and four, after students have gained some experience, but before they've set too many habits. For other courses, I expect the first week would be better.

_How many times a semester should we have the conversation?_ Student attitudes may change. Students may also need reminders. For CSC-151, I've often had to remind students what it means to be a good partner.

_Who gets to decide the policy?_ Does this end with "You may think that it's okay to do P, but I'm not going to permit it because Q" or does it end with "That's not what I would have chosen, and here's why not, but we'll go with the majority"?

_In how many courses should we have this assignment?_ I worry that the assignment might become a bit too repetitive if we do it in every course. However, different courses may have very different goals, and what is appropriate in one class may be inappropriate in another (or vice versa).

_How many scenarios are appropriate?_ I've written about twenty-four. That's a lot. As we've seen elsewhere, I tend to overdo things. At the same time, I see value in introducing students to ways in which one might use generative AI so that we can discover together that there's not one easy answer.

There are almost certainly others that will arise as I attempt to implement this idea. Fortunately (or unfortunately), I'm not teaching a full course again until Fall 2026, so I have some time. Perhaps I can convince a colleague to try this out in the meantime.

---

**_Postscript_**: Almost immediately after posting this musing, I discovered that my colleagues also include "Figuring out how to use a tool" and "Figuring out what may be wrong with a tool" (e.g., Visual Studio Code, the Java Compiler) as areas in which the use of AI may be permitted.
